{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for CPI prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Third-party library imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPI</th>\n",
       "      <th>Interest rate</th>\n",
       "      <th>GDP growth</th>\n",
       "      <th>Total pay nominal</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>USD/GBP</th>\n",
       "      <th>12 Month MA - current</th>\n",
       "      <th>M3 growth</th>\n",
       "      <th>Brent</th>\n",
       "      <th>CPI 3 Months ahead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.555256</td>\n",
       "      <td>0.516949</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.311263</td>\n",
       "      <td>0.503960</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.016617</td>\n",
       "      <td>0.080357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.555256</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.310624</td>\n",
       "      <td>0.509901</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.560647</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.332872</td>\n",
       "      <td>0.523762</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.064282</td>\n",
       "      <td>0.107143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.560647</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.349798</td>\n",
       "      <td>0.586139</td>\n",
       "      <td>0.385714</td>\n",
       "      <td>0.060959</td>\n",
       "      <td>0.098214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.560647</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.378539</td>\n",
       "      <td>0.599010</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.049851</td>\n",
       "      <td>0.098214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.544474</td>\n",
       "      <td>0.940678</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.149031</td>\n",
       "      <td>0.832673</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.592706</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.544474</td>\n",
       "      <td>0.923729</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.115180</td>\n",
       "      <td>0.799010</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.658912</td>\n",
       "      <td>0.366071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>4.6</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.539084</td>\n",
       "      <td>0.855932</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.091654</td>\n",
       "      <td>0.984158</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.631625</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>3.9</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.539084</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.119544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.564632</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.539084</td>\n",
       "      <td>0.737288</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.143283</td>\n",
       "      <td>0.923762</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.518191</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CPI  Interest rate  GDP growth  Total pay nominal  Unemployment  \\\n",
       "11   1.5       0.690265    0.555256           0.516949      0.326531   \n",
       "12   1.5       0.690265    0.555256           0.500000      0.326531   \n",
       "13   1.4       0.690265    0.560647           0.500000      0.326531   \n",
       "14   0.8       0.690265    0.560647           0.525424      0.326531   \n",
       "15   0.6       0.690265    0.560647           0.559322      0.326531   \n",
       "..   ...            ...         ...                ...           ...   \n",
       "269  6.7       0.911504    0.544474           0.940678      0.102041   \n",
       "270  6.7       0.911504    0.544474           0.923729      0.081633   \n",
       "271  4.6       0.911504    0.539084           0.855932      0.061224   \n",
       "272  3.9       0.911504    0.539084           0.813559      0.040816   \n",
       "273  4.0       0.911504    0.539084           0.737288      0.040816   \n",
       "\n",
       "      USD/GBP  12 Month MA - current  M3 growth     Brent  CPI 3 Months ahead  \n",
       "11   0.311263               0.503960   0.357143  0.016617            0.080357  \n",
       "12   0.310624               0.509901   0.371429  0.046528            0.062500  \n",
       "13   0.332872               0.523762   0.300000  0.064282            0.107143  \n",
       "14   0.349798               0.586139   0.385714  0.060959            0.098214  \n",
       "15   0.378539               0.599010   0.257143  0.049851            0.098214  \n",
       "..        ...                    ...        ...       ...                 ...  \n",
       "269  0.149031               0.832673   0.342857  0.592706            0.357143  \n",
       "270  0.115180               0.799010   0.228571  0.658912            0.366071  \n",
       "271  0.091654               0.984158   0.157143  0.631625                 NaN  \n",
       "272  0.119544               1.000000   0.400000  0.564632                 NaN  \n",
       "273  0.143283               0.923762   0.457143  0.518191                 NaN  \n",
       "\n",
       "[263 rows x 10 columns]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('Monthly_data.csv')\n",
    "\n",
    "# Preprocess data\n",
    "# Remove unnecessary columns and handle missing values\n",
    "data.drop(columns=['12 Month MA / Current Inflation'], inplace=True)  # Drop unwanted column\n",
    "data.drop(columns=['Year and Month'], inplace=True)  # Drop unwanted column\n",
    "data.dropna(inplace=True)  # Remove rows with missing values\n",
    "\n",
    "# Setup for prediction task\n",
    "lag = 3  # Number of months ahead to predict\n",
    "column_name = f'CPI {lag} Month{\"s\" if lag > 1 else \"\"} ahead'\n",
    "data[column_name] = data['CPI'].shift(-lag)  # Target variable for prediction\n",
    "\n",
    "# Select features and target for modeling\n",
    "# Excludes 'Year and Month' from features and uses dynamically selected last column as the target\n",
    "features = data.columns[1:-1]  # Features selection\n",
    "target = data.columns[-1]  # Target variable\n",
    "\n",
    "# Initialize and apply MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data[features] = scaler.fit_transform(data[features])  # Scale features\n",
    "data[[target]] = scaler.fit_transform(data[[target]])  # Scale target variable separately\n",
    "\n",
    "# Now, `data` is ready for machine learning model input\n",
    "\n",
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data used to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (208, 10)\n",
      "Testing data shape: (52, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPI</th>\n",
       "      <th>Interest rate</th>\n",
       "      <th>GDP growth</th>\n",
       "      <th>Total pay nominal</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>USD/GBP</th>\n",
       "      <th>12 Month MA - current</th>\n",
       "      <th>M3 growth</th>\n",
       "      <th>Brent</th>\n",
       "      <th>CPI 3 Months ahead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.555256</td>\n",
       "      <td>0.516949</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.311263</td>\n",
       "      <td>0.503960</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.016617</td>\n",
       "      <td>0.080357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.555256</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.310624</td>\n",
       "      <td>0.509901</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.560647</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.332872</td>\n",
       "      <td>0.523762</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.064282</td>\n",
       "      <td>0.107143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.560647</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.349798</td>\n",
       "      <td>0.586139</td>\n",
       "      <td>0.385714</td>\n",
       "      <td>0.060959</td>\n",
       "      <td>0.098214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.560647</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.378539</td>\n",
       "      <td>0.599010</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.049851</td>\n",
       "      <td>0.098214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>8.7</td>\n",
       "      <td>0.778761</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.855932</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.124122</td>\n",
       "      <td>0.674257</td>\n",
       "      <td>0.328571</td>\n",
       "      <td>0.499300</td>\n",
       "      <td>0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.867257</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.957627</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.140941</td>\n",
       "      <td>0.754455</td>\n",
       "      <td>0.328571</td>\n",
       "      <td>0.493790</td>\n",
       "      <td>0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.867257</td>\n",
       "      <td>0.544474</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.167554</td>\n",
       "      <td>0.852475</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.539881</td>\n",
       "      <td>0.419643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.544474</td>\n",
       "      <td>0.940678</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.149031</td>\n",
       "      <td>0.832673</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.592706</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.544474</td>\n",
       "      <td>0.923729</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.115180</td>\n",
       "      <td>0.799010</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.658912</td>\n",
       "      <td>0.366071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CPI  Interest rate  GDP growth  Total pay nominal  Unemployment  \\\n",
       "11   1.5       0.690265    0.555256           0.516949      0.326531   \n",
       "12   1.5       0.690265    0.555256           0.500000      0.326531   \n",
       "13   1.4       0.690265    0.560647           0.500000      0.326531   \n",
       "14   0.8       0.690265    0.560647           0.525424      0.326531   \n",
       "15   0.6       0.690265    0.560647           0.559322      0.326531   \n",
       "..   ...            ...         ...                ...           ...   \n",
       "266  8.7       0.778761    0.547170           0.855932      0.122449   \n",
       "267  7.9       0.867257    0.547170           0.957627      0.142857   \n",
       "268  6.8       0.867257    0.544474           0.966102      0.122449   \n",
       "269  6.7       0.911504    0.544474           0.940678      0.102041   \n",
       "270  6.7       0.911504    0.544474           0.923729      0.081633   \n",
       "\n",
       "      USD/GBP  12 Month MA - current  M3 growth     Brent  CPI 3 Months ahead  \n",
       "11   0.311263               0.503960   0.357143  0.016617            0.080357  \n",
       "12   0.310624               0.509901   0.371429  0.046528            0.062500  \n",
       "13   0.332872               0.523762   0.300000  0.064282            0.107143  \n",
       "14   0.349798               0.586139   0.385714  0.060959            0.098214  \n",
       "15   0.378539               0.599010   0.257143  0.049851            0.098214  \n",
       "..        ...                    ...        ...       ...                 ...  \n",
       "266  0.124122               0.674257   0.328571  0.499300            0.607143  \n",
       "267  0.140941               0.754455   0.328571  0.493790            0.607143  \n",
       "268  0.167554               0.852475   0.214286  0.539881            0.419643  \n",
       "269  0.149031               0.832673   0.342857  0.592706            0.357143  \n",
       "270  0.115180               0.799010   0.228571  0.658912            0.366071  \n",
       "\n",
       "[260 rows x 10 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameters for the sequence and prediction\n",
    "sequence_length = 20  # The length of input sequences for the model\n",
    "test_split = 0.2  # Fraction of the data to be used as the test set\n",
    "\n",
    "# Copy the original DataFrame to preserve the original data\n",
    "data_for_prediction = data.copy()\n",
    "\n",
    "# Select the final sequence of data for making future predictions\n",
    "# This dataset will be used to predict the future value(s) outside of the available dataset\n",
    "data_for_prediction = data_for_prediction.iloc[-( sequence_length + lag - 1):, :]\n",
    "\n",
    "# Prepare the dataset for training and testing the model\n",
    "# Exclude the last part used for prediction to ensure we are only working with historical data\n",
    "data_for_model = data.iloc[:-lag , :]\n",
    "\n",
    "# Split the historical data into training and testing datasets\n",
    "# Determine the index to split the data\n",
    "test_data_size = int(len(data_for_model) * test_split)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = data_for_model[:-test_data_size]\n",
    "test_data = data_for_model[-test_data_size:]\n",
    "\n",
    "# Display the prepared training data\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Testing data shape:\", test_data.shape)\n",
    "\n",
    "data_for_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the sequences from the features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data sequences shape: (188, 20, 8)\n",
      "Test data sequences shape: (32, 20, 8)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(data, input_columns, target_column, sequence_length):\n",
    "    \"\"\"Create sequences of input features and target values.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        # Extract the sequence of input features\n",
    "        X.append(data[input_columns].iloc[i:i+sequence_length].values)\n",
    "        # Extract the target value following the sequence\n",
    "        y.append(data[target_column].iloc[i+sequence_length])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Specify the input and target columns\n",
    "input_columns = features \n",
    "target_column = target  \n",
    "\n",
    "# Generate sequences for the LSTM model\n",
    "X_train, y_train = create_sequences(train_data, input_columns, target_column, sequence_length)\n",
    "X_test, y_test = create_sequences(test_data, input_columns, target_column, sequence_length)\n",
    "\n",
    "# Output the shape of the training data sequences for verification\n",
    "print(\"Training data sequences shape:\", X_train.shape)\n",
    "print(\"Test data sequences shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 - One LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_253\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_253 (LSTM)             (None, 100)               43600     \n",
      "                                                                 \n",
      " dense_253 (Dense)           (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43701 (170.71 KB)\n",
      "Trainable params: 43701 (170.71 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "6/6 [==============================] - 3s 86ms/step - loss: 0.0452 - val_loss: 0.0169\n",
      "Epoch 2/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.0145 - val_loss: 0.0022\n",
      "Epoch 3/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.0170 - val_loss: 2.4122e-04\n",
      "Epoch 4/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0102 - val_loss: 0.0020\n",
      "Epoch 5/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.0099 - val_loss: 2.7321e-04\n",
      "Epoch 6/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0091 - val_loss: 0.0015\n",
      "Epoch 7/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0083 - val_loss: 4.1090e-04\n",
      "Epoch 8/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0078 - val_loss: 5.0187e-04\n",
      "Epoch 9/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0072 - val_loss: 0.0017\n",
      "Epoch 10/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0069 - val_loss: 0.0014\n",
      "Epoch 11/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0067 - val_loss: 0.0015\n",
      "Epoch 12/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0062 - val_loss: 0.0022\n",
      "Epoch 13/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0059 - val_loss: 0.0020\n",
      "Epoch 14/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0057 - val_loss: 0.0023\n",
      "Epoch 15/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0052 - val_loss: 0.0026\n",
      "Epoch 16/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0046 - val_loss: 0.0030\n",
      "Epoch 17/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0043 - val_loss: 0.0034\n",
      "Epoch 18/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0037 - val_loss: 0.0016\n",
      "Epoch 19/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 20/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0029 - val_loss: 0.0041\n",
      "Epoch 21/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0044 - val_loss: 0.0016\n",
      "Epoch 22/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0037 - val_loss: 4.3940e-04\n",
      "Epoch 23/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0029 - val_loss: 0.0017\n",
      "Epoch 24/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0026 - val_loss: 4.7563e-04\n",
      "Epoch 25/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0026 - val_loss: 7.7817e-04\n",
      "Epoch 26/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0026 - val_loss: 8.5043e-04\n",
      "Epoch 27/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0024 - val_loss: 7.7945e-04\n",
      "Epoch 28/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0026 - val_loss: 8.0496e-04\n",
      "Epoch 29/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0028 - val_loss: 8.7843e-04\n",
      "Epoch 30/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0027 - val_loss: 6.4556e-04\n",
      "Epoch 31/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0023 - val_loss: 9.3775e-04\n",
      "Epoch 32/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0028 - val_loss: 8.5411e-04\n",
      "Epoch 33/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 34/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0026 - val_loss: 6.1332e-04\n",
      "Epoch 35/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0026 - val_loss: 7.5646e-04\n",
      "Epoch 36/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0027 - val_loss: 0.0013\n",
      "Epoch 37/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0026 - val_loss: 8.3025e-04\n",
      "Epoch 38/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0029 - val_loss: 0.0020\n",
      "Epoch 39/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0028 - val_loss: 0.0010\n",
      "Epoch 40/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0026 - val_loss: 0.0020\n",
      "Epoch 41/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0025 - val_loss: 8.1526e-04\n",
      "Epoch 42/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 43/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0020 - val_loss: 8.5547e-04\n",
      "Epoch 44/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0021 - val_loss: 9.3980e-04\n",
      "Epoch 45/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 46/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0026 - val_loss: 8.9949e-04\n",
      "Epoch 47/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 48/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0022 - val_loss: 0.0010\n",
      "Epoch 49/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 50/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 51/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 52/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 53/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0016 - val_loss: 9.5030e-04\n",
      "Epoch 54/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 55/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0022 - val_loss: 0.0011\n",
      "Epoch 56/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 57/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 58/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 59/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 60/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0023 - val_loss: 0.0019\n",
      "Epoch 61/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 62/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0023 - val_loss: 0.0039\n",
      "Epoch 63/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0019 - val_loss: 9.2705e-04\n",
      "Epoch 64/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 65/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 66/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 67/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0019 - val_loss: 0.0030\n",
      "Epoch 68/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0018 - val_loss: 0.0038\n",
      "Epoch 69/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 70/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 71/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 72/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 73/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0017 - val_loss: 0.0031\n",
      "Epoch 74/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 75/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 76/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0018 - val_loss: 0.0030\n",
      "Epoch 77/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 78/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0022 - val_loss: 0.0049\n",
      "Epoch 79/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 80/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0019 - val_loss: 0.0037\n",
      "Epoch 81/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 82/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 83/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 84/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 85/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 86/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 87/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 88/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 89/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 90/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 91/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 92/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 93/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.0016 - val_loss: 0.0037\n",
      "Epoch 94/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.0018 - val_loss: 0.0029\n",
      "Epoch 95/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 96/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0018 - val_loss: 0.0037\n",
      "Epoch 97/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 98/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 99/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 100/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 101/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0020 - val_loss: 0.0028\n",
      "Epoch 102/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 103/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 104/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 105/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 106/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 107/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0017 - val_loss: 0.0027\n",
      "Epoch 108/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 109/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0017 - val_loss: 0.0034\n",
      "Epoch 110/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 111/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0014 - val_loss: 0.0029\n",
      "Epoch 112/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0018 - val_loss: 0.0028\n",
      "Epoch 113/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 114/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 115/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0013 - val_loss: 0.0031\n",
      "Epoch 116/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 117/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0016 - val_loss: 0.0031\n",
      "Epoch 118/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 119/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 120/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 121/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 122/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0017 - val_loss: 0.0038\n",
      "Epoch 123/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 124/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0017 - val_loss: 0.0029\n",
      "Epoch 125/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0015 - val_loss: 0.0031\n",
      "Epoch 126/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 127/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 128/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0016 - val_loss: 0.0042\n",
      "Epoch 129/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0018 - val_loss: 0.0039\n",
      "Epoch 130/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 131/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.0018 - val_loss: 0.0027\n",
      "Epoch 132/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 133/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0013 - val_loss: 0.0031\n",
      "Epoch 134/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 135/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 136/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 137/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 138/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 139/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 140/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 141/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 142/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 143/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 144/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 145/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 146/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 147/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0014 - val_loss: 0.0023\n",
      "Epoch 148/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0014 - val_loss: 0.0032\n",
      "Epoch 149/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0013 - val_loss: 0.0028\n",
      "Epoch 150/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0013 - val_loss: 0.0035\n",
      "Epoch 151/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0017 - val_loss: 0.0038\n",
      "Epoch 152/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0014 - val_loss: 0.0029\n",
      "Epoch 153/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 154/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 155/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 156/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0014 - val_loss: 0.0032\n",
      "Epoch 157/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0014 - val_loss: 0.0032\n",
      "Epoch 158/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 159/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 160/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 161/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 162/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0013 - val_loss: 0.0028\n",
      "Epoch 163/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0014 - val_loss: 0.0028\n",
      "Epoch 164/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 165/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 166/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0015 - val_loss: 0.0036\n",
      "Epoch 167/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 168/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0015 - val_loss: 0.0028\n",
      "Epoch 169/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 170/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 171/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0018 - val_loss: 0.0041\n",
      "Epoch 172/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 173/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0013 - val_loss: 0.0030\n",
      "Epoch 174/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 175/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 176/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0018 - val_loss: 0.0045\n",
      "Epoch 177/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 178/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0015 - val_loss: 0.0037\n",
      "Epoch 179/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 180/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0014 - val_loss: 0.0044\n",
      "Epoch 181/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0016 - val_loss: 0.0038\n",
      "Epoch 182/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0013 - val_loss: 0.0027\n",
      "Epoch 183/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0016 - val_loss: 0.0029\n",
      "Epoch 184/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 185/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0015 - val_loss: 0.0030\n",
      "Epoch 186/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0011 - val_loss: 0.0024\n",
      "Epoch 187/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0014 - val_loss: 0.0039\n",
      "Epoch 188/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0012 - val_loss: 0.0023\n",
      "Epoch 189/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0013 - val_loss: 0.0030\n",
      "Epoch 190/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0011 - val_loss: 0.0031\n",
      "Epoch 191/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0033\n",
      "Epoch 192/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0013 - val_loss: 0.0037\n",
      "Epoch 193/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0012 - val_loss: 0.0046\n",
      "Epoch 194/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.6440e-04 - val_loss: 0.0023\n",
      "Epoch 195/500\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0013 - val_loss: 0.0025\n",
      "Epoch 196/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0013 - val_loss: 0.0033\n",
      "Epoch 197/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0036\n",
      "Epoch 198/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 199/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 200/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0012 - val_loss: 0.0031\n",
      "Epoch 201/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0013 - val_loss: 0.0030\n",
      "Epoch 202/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0028\n",
      "Epoch 203/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0026\n",
      "Epoch 204/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0012 - val_loss: 0.0048\n",
      "Epoch 205/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0012 - val_loss: 0.0027\n",
      "Epoch 206/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0012 - val_loss: 0.0026\n",
      "Epoch 207/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 9.8758e-04 - val_loss: 0.0027\n",
      "Epoch 208/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 209/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.7661e-04 - val_loss: 0.0028\n",
      "Epoch 210/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0012 - val_loss: 0.0026\n",
      "Epoch 211/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0028\n",
      "Epoch 212/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0013 - val_loss: 0.0035\n",
      "Epoch 213/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0015 - val_loss: 0.0044\n",
      "Epoch 214/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0026\n",
      "Epoch 215/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0011 - val_loss: 0.0025\n",
      "Epoch 216/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0013 - val_loss: 0.0027\n",
      "Epoch 217/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0011 - val_loss: 0.0045\n",
      "Epoch 218/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 219/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0012 - val_loss: 0.0027\n",
      "Epoch 220/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0015 - val_loss: 0.0036\n",
      "Epoch 221/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 222/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0013 - val_loss: 0.0028\n",
      "Epoch 223/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0050\n",
      "Epoch 224/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0011 - val_loss: 0.0035\n",
      "Epoch 225/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0012 - val_loss: 0.0042\n",
      "Epoch 226/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0041\n",
      "Epoch 227/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0010 - val_loss: 0.0022\n",
      "Epoch 228/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0043\n",
      "Epoch 229/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 230/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0012 - val_loss: 0.0043\n",
      "Epoch 231/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 232/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0016 - val_loss: 0.0037\n",
      "Epoch 233/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 234/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 235/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 236/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 237/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0017 - val_loss: 0.0036\n",
      "Epoch 238/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0013 - val_loss: 0.0036\n",
      "Epoch 239/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0013 - val_loss: 0.0040\n",
      "Epoch 240/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0010 - val_loss: 0.0020\n",
      "Epoch 241/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 242/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 243/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0014 - val_loss: 0.0043\n",
      "Epoch 244/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 245/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 246/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0013 - val_loss: 0.0033\n",
      "Epoch 247/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 248/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0013 - val_loss: 0.0031\n",
      "Epoch 249/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 250/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 251/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0028\n",
      "Epoch 252/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.0012 - val_loss: 0.0025\n",
      "Epoch 253/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0013 - val_loss: 0.0039\n",
      "Epoch 254/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 255/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 256/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0012 - val_loss: 0.0025\n",
      "Epoch 257/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0037\n",
      "Epoch 258/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0014 - val_loss: 0.0038\n",
      "Epoch 259/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0027\n",
      "Epoch 260/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0012 - val_loss: 0.0027\n",
      "Epoch 261/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0044\n",
      "Epoch 262/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 9.9915e-04 - val_loss: 0.0025\n",
      "Epoch 263/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0056\n",
      "Epoch 264/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 265/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0016 - val_loss: 0.0049\n",
      "Epoch 266/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 267/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0012 - val_loss: 0.0029\n",
      "Epoch 268/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0045\n",
      "Epoch 269/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0011 - val_loss: 0.0043\n",
      "Epoch 270/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0036\n",
      "Epoch 271/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 272/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 273/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0013 - val_loss: 0.0027\n",
      "Epoch 274/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0036\n",
      "Epoch 275/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0045\n",
      "Epoch 276/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0023\n",
      "Epoch 277/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0012 - val_loss: 0.0031\n",
      "Epoch 278/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 279/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.0010 - val_loss: 0.0025\n",
      "Epoch 280/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 281/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.4594e-04 - val_loss: 0.0028\n",
      "Epoch 282/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 9.2969e-04 - val_loss: 0.0029\n",
      "Epoch 283/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0024\n",
      "Epoch 284/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 285/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.6635e-04 - val_loss: 0.0017\n",
      "Epoch 286/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0012 - val_loss: 0.0031\n",
      "Epoch 287/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0012 - val_loss: 0.0030\n",
      "Epoch 288/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 289/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 290/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0028\n",
      "Epoch 291/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 9.3756e-04 - val_loss: 0.0028\n",
      "Epoch 292/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 9.7668e-04 - val_loss: 0.0033\n",
      "Epoch 293/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0017\n",
      "Epoch 294/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0011 - val_loss: 0.0031\n",
      "Epoch 295/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0010 - val_loss: 0.0027\n",
      "Epoch 296/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 297/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 298/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0010 - val_loss: 0.0017\n",
      "Epoch 299/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0034\n",
      "Epoch 300/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.8221e-04 - val_loss: 0.0028\n",
      "Epoch 301/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0037\n",
      "Epoch 302/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0021\n",
      "Epoch 303/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0023\n",
      "Epoch 304/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0011 - val_loss: 0.0028\n",
      "Epoch 305/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 9.6532e-04 - val_loss: 0.0036\n",
      "Epoch 306/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0029\n",
      "Epoch 307/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0022\n",
      "Epoch 308/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.1389e-04 - val_loss: 0.0024\n",
      "Epoch 309/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.7551e-04 - val_loss: 0.0032\n",
      "Epoch 310/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0028\n",
      "Epoch 311/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0010 - val_loss: 0.0023\n",
      "Epoch 312/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0012 - val_loss: 0.0041\n",
      "Epoch 313/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.8135e-04 - val_loss: 0.0028\n",
      "Epoch 314/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0012 - val_loss: 0.0034\n",
      "Epoch 315/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0023\n",
      "Epoch 316/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.8632e-04 - val_loss: 0.0039\n",
      "Epoch 317/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0027\n",
      "Epoch 318/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 8.4847e-04 - val_loss: 0.0023\n",
      "Epoch 319/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 320/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0010 - val_loss: 0.0044\n",
      "Epoch 321/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0011 - val_loss: 0.0026\n",
      "Epoch 322/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0010 - val_loss: 0.0032\n",
      "Epoch 323/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0011 - val_loss: 0.0026\n",
      "Epoch 324/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.5852e-04 - val_loss: 0.0015\n",
      "Epoch 325/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 326/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0013 - val_loss: 0.0034\n",
      "Epoch 327/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0011 - val_loss: 0.0028\n",
      "Epoch 328/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0012 - val_loss: 0.0031\n",
      "Epoch 329/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 330/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 331/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 332/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 333/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 334/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 335/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0029\n",
      "Epoch 336/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0010 - val_loss: 0.0021\n",
      "Epoch 337/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 338/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0011 - val_loss: 0.0031\n",
      "Epoch 339/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 340/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0010 - val_loss: 0.0022\n",
      "Epoch 341/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0023\n",
      "Epoch 342/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.0011 - val_loss: 0.0030\n",
      "Epoch 343/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0010 - val_loss: 0.0021\n",
      "Epoch 344/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.0220e-04 - val_loss: 0.0030\n",
      "Epoch 345/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.3430e-04 - val_loss: 0.0024\n",
      "Epoch 346/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.5869e-04 - val_loss: 0.0025\n",
      "Epoch 347/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 8.9527e-04 - val_loss: 0.0019\n",
      "Epoch 348/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.6350e-04 - val_loss: 0.0037\n",
      "Epoch 349/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.2962e-04 - val_loss: 0.0037\n",
      "Epoch 350/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 351/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0013 - val_loss: 0.0039\n",
      "Epoch 352/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0023\n",
      "Epoch 353/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0011 - val_loss: 0.0025\n",
      "Epoch 354/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 9.8708e-04 - val_loss: 0.0030\n",
      "Epoch 355/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.3270e-04 - val_loss: 0.0026\n",
      "Epoch 356/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.3156e-04 - val_loss: 0.0030\n",
      "Epoch 357/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0025\n",
      "Epoch 358/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.4085e-04 - val_loss: 0.0027\n",
      "Epoch 359/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.8233e-04 - val_loss: 0.0025\n",
      "Epoch 360/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 9.9799e-04 - val_loss: 0.0031\n",
      "Epoch 361/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 8.7353e-04 - val_loss: 0.0025\n",
      "Epoch 362/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 8.6370e-04 - val_loss: 0.0020\n",
      "Epoch 363/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 8.5075e-04 - val_loss: 0.0029\n",
      "Epoch 364/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.0010 - val_loss: 0.0021\n",
      "Epoch 365/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 9.4507e-04 - val_loss: 0.0027\n",
      "Epoch 366/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 8.0496e-04 - val_loss: 0.0030\n",
      "Epoch 367/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 9.6722e-04 - val_loss: 0.0038\n",
      "Epoch 368/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.9204e-04 - val_loss: 0.0041\n",
      "Epoch 369/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0023\n",
      "Epoch 370/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.5425e-04 - val_loss: 0.0035\n",
      "Epoch 371/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 372/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 9.9946e-04 - val_loss: 0.0026\n",
      "Epoch 373/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 9.2635e-04 - val_loss: 0.0029\n",
      "Epoch 374/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 9.6502e-04 - val_loss: 0.0023\n",
      "Epoch 375/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0011 - val_loss: 0.0023\n",
      "Epoch 376/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 8.5324e-04 - val_loss: 0.0034\n",
      "Epoch 377/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.6823e-04 - val_loss: 0.0026\n",
      "Epoch 378/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 7.9994e-04 - val_loss: 0.0039\n",
      "Epoch 379/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.5485e-04 - val_loss: 0.0036\n",
      "Epoch 380/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 9.7869e-04 - val_loss: 0.0020\n",
      "Epoch 381/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.9613e-04 - val_loss: 0.0031\n",
      "Epoch 382/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.6039e-04 - val_loss: 0.0023\n",
      "Epoch 383/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.2498e-04 - val_loss: 0.0027\n",
      "Epoch 384/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0011 - val_loss: 0.0033\n",
      "Epoch 385/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.9970e-04 - val_loss: 0.0018\n",
      "Epoch 386/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 9.8383e-04 - val_loss: 0.0017\n",
      "Epoch 387/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 8.3412e-04 - val_loss: 0.0021\n",
      "Epoch 388/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 9.0521e-04 - val_loss: 0.0021\n",
      "Epoch 389/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.3230e-04 - val_loss: 0.0021\n",
      "Epoch 390/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.9977e-04 - val_loss: 0.0023\n",
      "Epoch 391/500\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 392/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 7.8450e-04 - val_loss: 0.0026\n",
      "Epoch 393/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 394/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 9.4570e-04 - val_loss: 0.0016\n",
      "Epoch 395/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 396/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.9516e-04 - val_loss: 0.0020\n",
      "Epoch 397/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.9348e-04 - val_loss: 0.0020\n",
      "Epoch 398/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.9163e-04 - val_loss: 0.0022\n",
      "Epoch 399/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 400/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.3395e-04 - val_loss: 0.0016\n",
      "Epoch 401/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 7.7006e-04 - val_loss: 0.0025\n",
      "Epoch 402/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 403/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 8.9356e-04 - val_loss: 0.0018\n",
      "Epoch 404/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 405/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 406/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 407/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.3404e-04 - val_loss: 0.0025\n",
      "Epoch 408/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 7.9611e-04 - val_loss: 0.0030\n",
      "Epoch 409/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.9456e-04 - val_loss: 0.0015\n",
      "Epoch 410/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 9.2814e-04 - val_loss: 0.0018\n",
      "Epoch 411/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 9.1058e-04 - val_loss: 0.0016\n",
      "Epoch 412/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.1976e-04 - val_loss: 0.0026\n",
      "Epoch 413/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.8055e-04 - val_loss: 0.0024\n",
      "Epoch 414/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 8.5199e-04 - val_loss: 0.0019\n",
      "Epoch 415/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 9.2364e-04 - val_loss: 0.0022\n",
      "Epoch 416/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 417/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.9328e-04 - val_loss: 0.0034\n",
      "Epoch 418/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.3487e-04 - val_loss: 0.0019\n",
      "Epoch 419/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 420/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 7.9352e-04 - val_loss: 0.0016\n",
      "Epoch 421/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 8.6344e-04 - val_loss: 0.0014\n",
      "Epoch 422/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.3198e-04 - val_loss: 0.0035\n",
      "Epoch 423/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 424/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 9.1819e-04 - val_loss: 0.0013\n",
      "Epoch 425/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 9.6889e-04 - val_loss: 0.0018\n",
      "Epoch 426/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 427/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0010 - val_loss: 0.0019\n",
      "Epoch 428/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 9.4776e-04 - val_loss: 0.0011\n",
      "Epoch 429/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.0231e-04 - val_loss: 0.0015\n",
      "Epoch 430/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.2444e-04 - val_loss: 0.0021\n",
      "Epoch 431/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.0783e-04 - val_loss: 0.0025\n",
      "Epoch 432/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.6664e-04 - val_loss: 0.0016\n",
      "Epoch 433/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 434/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.2830e-04 - val_loss: 0.0015\n",
      "Epoch 435/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 436/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 8.7230e-04 - val_loss: 0.0018\n",
      "Epoch 437/500\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 8.5164e-04 - val_loss: 0.0012\n",
      "Epoch 438/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 8.4189e-04 - val_loss: 0.0012\n",
      "Epoch 439/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0010 - val_loss: 0.0021\n",
      "Epoch 440/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 8.9666e-04 - val_loss: 0.0012\n",
      "Epoch 441/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 442/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.6892e-04 - val_loss: 0.0013\n",
      "Epoch 443/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 7.9893e-04 - val_loss: 0.0014\n",
      "Epoch 444/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.9981e-04 - val_loss: 0.0013\n",
      "Epoch 445/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.5946e-04 - val_loss: 0.0017\n",
      "Epoch 446/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.6009e-04 - val_loss: 0.0019\n",
      "Epoch 447/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 448/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 9.3489e-04 - val_loss: 0.0012\n",
      "Epoch 449/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 9.4380e-04 - val_loss: 0.0020\n",
      "Epoch 450/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 451/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 7.6252e-04 - val_loss: 0.0012\n",
      "Epoch 452/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 9.5539e-04 - val_loss: 0.0014\n",
      "Epoch 453/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 454/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 7.9932e-04 - val_loss: 0.0011\n",
      "Epoch 455/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 9.6887e-04 - val_loss: 0.0011\n",
      "Epoch 456/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.3561e-04 - val_loss: 0.0014\n",
      "Epoch 457/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.1955e-04 - val_loss: 0.0018\n",
      "Epoch 458/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.7341e-04 - val_loss: 0.0015\n",
      "Epoch 459/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 460/500\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 9.5913e-04 - val_loss: 0.0011\n",
      "Epoch 461/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 8.7357e-04 - val_loss: 0.0014\n",
      "Epoch 462/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 7.4655e-04 - val_loss: 0.0017\n",
      "Epoch 463/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.5122e-04 - val_loss: 0.0015\n",
      "Epoch 464/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.5239e-04 - val_loss: 0.0013\n",
      "Epoch 465/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 9.3073e-04 - val_loss: 0.0024\n",
      "Epoch 466/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.6748e-04 - val_loss: 0.0033\n",
      "Epoch 467/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 468/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 9.4089e-04 - val_loss: 0.0011\n",
      "Epoch 469/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 470/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 471/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 9.1619e-04 - val_loss: 0.0014\n",
      "Epoch 472/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 473/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.7493e-04 - val_loss: 0.0019\n",
      "Epoch 474/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 9.3084e-04 - val_loss: 0.0018\n",
      "Epoch 475/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 7.9672e-04 - val_loss: 0.0018\n",
      "Epoch 476/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 7.7870e-04 - val_loss: 0.0017\n",
      "Epoch 477/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 7.4714e-04 - val_loss: 0.0015\n",
      "Epoch 478/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 7.4927e-04 - val_loss: 0.0025\n",
      "Epoch 479/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.6229e-04 - val_loss: 0.0015\n",
      "Epoch 480/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 481/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.8468e-04 - val_loss: 0.0011\n",
      "Epoch 482/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0010 - val_loss: 0.0033\n",
      "Epoch 483/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 484/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.2566e-04 - val_loss: 0.0013\n",
      "Epoch 485/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.7162e-04 - val_loss: 0.0012\n",
      "Epoch 486/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 6.4074e-04 - val_loss: 0.0014\n",
      "Epoch 487/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 7.9059e-04 - val_loss: 0.0027\n",
      "Epoch 488/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 489/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 9.6469e-04 - val_loss: 0.0018\n",
      "Epoch 490/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 491/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0010 - val_loss: 0.0019\n",
      "Epoch 492/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 7.6608e-04 - val_loss: 0.0014\n",
      "Epoch 493/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 7.4832e-04 - val_loss: 0.0018\n",
      "Epoch 494/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 8.2894e-04 - val_loss: 0.0012\n",
      "Epoch 495/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 496/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 9.9030e-04 - val_loss: 0.0016\n",
      "Epoch 497/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 7.5885e-04 - val_loss: 0.0020\n",
      "Epoch 498/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 9.8697e-04 - val_loss: 0.0024\n",
      "Epoch 499/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 7.2001e-04 - val_loss: 0.0012\n",
      "Epoch 500/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.6392e-04 - val_loss: 0.0023\n",
      "Training Loss: 0.0006395263480953872\n",
      "Testing Loss: 0.1995181292295456\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(100, activation='tanh', recurrent_activation='sigmoid',\n",
    "         input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "         dropout=0.05, recurrent_dropout=0.05),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Trying the Adam optimizer\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "'''\n",
    "# Split your data into training and validation sets\n",
    "X_train_part, X_val, y_train_part, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "# Then, use the `validation_data` parameter in `model.fit()`\n",
    "history = model.fit(X_train_part, y_train_part, epochs=100, validation_data=(X_val, y_val), batch_size=32)\n",
    "'''\n",
    "\n",
    "# Include validation split for training monitoring\n",
    "history = model.fit(X_train, y_train, epochs=500, validation_split=0.05, batch_size=32)\n",
    "\n",
    "train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Training Loss: {train_loss}')\n",
    "print(f'Testing Loss: {test_loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 - Two LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Model architecture\n",
    "\n",
    "model = Sequential([\n",
    "    # First LSTM layer returns sequences to feed into the next LSTM layer\n",
    "    LSTM(20, activation='tanh', input_shape=(X_test.shape[1], X_test.shape[2]), return_sequences=True),\n",
    "    # Second LSTM layer only needs to return the last output\n",
    "    LSTM(2, activation='relu'),\n",
    "    # Followed by a Dense layer that makes the final prediction\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=32)\n",
    "\n",
    "train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Training Loss: {train_loss}')\n",
    "print(f'Testing Loss: {test_loss}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration: 1/3\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 3\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    print(f\"Running iteration: {i+1}/{n_iterations}\")\n",
    "    \n",
    "    # Re-initialize the model at each iteration\n",
    "    model = Sequential([\n",
    "        LSTM(200, activation='tanh', recurrent_activation='sigmoid',\n",
    "             input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "             dropout=0.05, recurrent_dropout=0.05),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, epochs=200, validation_split=0.0, batch_size=32, verbose=0)  # Set verbose=0 for less output\n",
    "    \n",
    "    # Evaluate the model\n",
    "    train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    # Store losses\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "# Calculate the average losses\n",
    "average_train_loss = np.mean(train_losses)\n",
    "average_test_loss = np.mean(test_losses)\n",
    "\n",
    "print(f'Average Training Loss: {average_train_loss}')\n",
    "print(f'Average Testing Loss: {average_test_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visuallise training loss vs validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 349ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.304046]], dtype=float32)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full, y_full = create_sequences(data_for_model, input_columns, target_column, sequence_length)\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(200, activation='tanh', recurrent_activation='sigmoid',\n",
    "         input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "         dropout=0.1, recurrent_dropout=0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Trying the Adam optimizer\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.fit(X_full, y_full, epochs=200, validation_split=0.0, batch_size=32, verbose=0)\n",
    "\n",
    "def create_sequences_X(data, input_columns, sequence_length):\n",
    "    X= []\n",
    "    for i in range(lag):\n",
    "        X.append(data[input_columns].iloc[i:i+sequence_length].values)\n",
    "    \n",
    "    return np.array(X)\n",
    "\n",
    "input_columns = features\n",
    "\n",
    "X_prediction = create_sequences_X(data_for_prediction, input_columns, sequence_length)\n",
    "\n",
    "predicted_cpi = model.predict(X_prediction)\n",
    "\n",
    "predicted_cpi_inverted = scaler.inverse_transform(predicted_cpi)\n",
    "\n",
    "predicted_cpi_inverted\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def build_and_evaluate_model(n_units, X_train, y_train, X_test, y_test):\n",
    "    model = Sequential([\n",
    "        LSTM(n_units, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='sgd', loss='mse')\n",
    "    model.fit(X_train, y_train, epochs=1000, validation_split=0.006, batch_size=32, verbose=0)\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return test_loss\n",
    "\n",
    "units_list = list([50,100, 150,200, 250, 300, 350, 400, 450, 500])\n",
    "test_losses = []\n",
    "\n",
    "for units in units_list:\n",
    "    test_loss = build_and_evaluate_model(units, X_train, y_train, X_test, y_test)\n",
    "    test_losses.append(test_loss)\n",
    "    print(f\"Test loss with {units} units: {test_loss}\")\n",
    "\n",
    "# Plotting the test loss vs. number of units\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(units_list, test_losses, marker='o', linestyle='-', color='b')\n",
    "plt.title('Test Loss vs. Number of LSTM Units')\n",
    "plt.xlabel('Number of LSTM Units')\n",
    "plt.ylabel('Test Loss (MSE)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
